---
permalink: /
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a research assistant at [Shanghai AI Lab](https://www.shlab.org.cn), working with Dong Wang and [Chenjia Bai](https://baichenjia.github.io). Prior to this, I obtained my **M.S. degree** in Control Science and Engineering from [National University of Defense Technology](https://www.nudt.edu.cn), advised by Prof. Dr. [Xin Xu](https://xueshu.baidu.com/scholarID/CN-B7736SUJ), and my bachelor's degree in Computer Science and Technology from [Sichuan University](https://www.scu.edu.cn).
    

  <p>
    <strong>Research Areas & Interests:</strong>
    My research areas lie in robotics, computer vision, and deep reinforcement learning. 
    
    In particular, I am interested in embodied intelligence with focus on representation learning for multi-modal perception and long-horizon planning.
  </p>
  + <font color='green'>Representation learning for multi-modal perception</font>
  When robots interact with real-world environments, they will receive multi-modal perception inputs, including images, audio, etc. How to learn object-generic and task-driven representations is critical for efficient skill learning. Specifically, can we represent objects of different materials (rigid, fluidic, deformable, particulate) in a generic form? How to use a target task as a prompt to learn task-efficient representations?
  + <font color='green'>Hierarchical embodied planning</font>
  Decoupling tasks to sequential atomic skills is demonstrated as a promising solution to tackle long-horizon planning tasks. Currently, many researchers leverage large language models (LLMs) to provide skill sequences, and then execute rule-based or learning-based skills. Based on these approaches, there are plenty of problems I’m interested in. For example, how to define and decide the granularity of skills? How to close the loop between high-level skill sequences and low-level control signals for learning-based skills? And how can we solve the “Hand over” problem of skill sequence?


Publications
------
[1] **Tang Z**, Shi Y, Xu X. CSGP: Closed-loop Safe Grasp Planning via Attention-based Deep Reinforcement Learning from Demonstrations[J]. IEEE Robotics and Automation Letters, 2023, 8(6): 3158-3165.
[2] Shi Y, **Tang Z**, Cai X, Zhang H, Hu D, Xu X. SymmetryGrasp: Symmetry-Aware Antipodal Grasp Detection From Single-View 
RGB-D Images[J]. IEEE Robotics and Automation Letters, 2022, 7(4):12235-12242.
[3] Xiao Y, **Tang Z**, Xu X, Zhang X, Shi Y. A deep Koopman operator-based modelling approach for long-term prediction of 
dynamics with pixel-level measurements[J]. CAAI Transactions on Intelligence Technology, 2023.
[4] Lan Y, Ren J, Tang T, Xu X, Shi Y, **Tang Z**. Efficient reinforcement learning with least-squares soft Bellman residual for robotic grasping[J]. Robotics and Autonomous Systems, 2023, 164: 104385.
[5] **Tang Z**, Xu X, Shi Y. Grasp Planning Based on Deep Reinforcement Learning: A Brief Survey[C]. China Automation Congress (CAC). 2021: 7293-7299.



For more info
------
More info can be found in [my Curriculum Vitae](https://Zixin-Tang.github.io/assets/Zixin Tang Curriculum Vitae.pdf).
